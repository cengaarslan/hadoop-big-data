/////////////////  NIFI KURULUMU ///////////////////////////////////////////
1. nifi repo dosyasını /etc/yum.repos.d/ diznine indir.

	cd /etc/yum.repos.d/
	wget	http://public-repo-1.hortonworks.com/HDF/centos6/3.x/updates/3.1.2.0/hdf.repo
	
2. yum info nifi* komutuyla paket kontrolü yap.

3. yum -y install  nifi_3_1_2_0_7 komutu ile kurulumu başlat.

4. vi /usr/hdf/current/nifi/conf/nifi.properties ile dosya içine gir.
	
	#web properties# bölümünde
	nifi.web.http.port= değerini 8080'den 8090'a çek.

	Açılması biraz zaman alır. 
	
	Çalışırlık kontrolü:
	 /usr/hdf/current/nifi/bin/nifi.sh status
	
5. kurulum sonrası nifi çalıştırma
	/usr/hdf/current/nifi/bin/nifi.sh start
	 
6. Browser’a http://sandbox.hortonworks.com:8090/nifi/ adresini gir ve NiFi arayüzünü gör. 


///////////////////// NiFi Mysql’den Kafka’ya incremental veri akışı ////////////////////////

 Mysql veri tabanında hazırlık yap.
 
1. Kullanılacak iris veri setini indir 
	cd /home/maria_dev
	wget http://veribilimi.co/data/iris_with_pk.csv 
	ile  /home/maria_dev diznine veri ardışık artan Id sütununa sahip iris veri setini indir.

2. Mysql’e bağlan, azhadoop veri tabanını seç
	
3. mysql> create table iris_with_pk(Id int  NOT NULL AUTO_INCREMENT, SepalLengthCm double, SepalWidthCm double, PetalLengthCm double, PetalWidthCm double, Species VARCHAR(20), PRIMARY KEY (Id));  
	komutuyla tablo yarat
	
4. mysql> load data local infile '/home/maria_dev/iris_with_pk.csv' into table iris_with_pk fields terminated by ',' enclosed by '"' lines terminated by '\n'  (Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species); 
	ile veri setini tabloya yükle
	

	Nifi Arayüzü Ön Hazırlıklar:
5.	Browser’a http://sandbox.hortonworks.com:8090/nifi/ adresini gir ve NiFi arayüzünü gör. 

6.	Nifi arayüzünden mysql veri tabanı bağlantısı için connection pool oluştur.
	 Ana sayfa -> sol menü configüration (küçük çark) -> controller service tabı -> sağ tarafta + işareti -> DBCPConnectionPool -> Configure
	 
	Properties tabına:
		Database connection url: jdbc:mysql://127.0.0.1/azhadoop
		Database Driver Class Name: com.mysql.jdbc.Driver
		Database Driver Location(s): /usr/share/java/mysql-connector-java.jar
		Database User: root
		Password: hadoop
		
Sağ taraftan enable et

 Kafka Topic Yaratma:
1. /usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --zookeeper sandbox.hortonworks.com:2181 --replication-factor 1 --partitions 1 --topic iris

1.	Kafka Producer:
/usr/hdp/current/kafka-broker/bin/kafka-console-producer.sh --broker-list sandbox.hortonworks.com:6667 --topic iris

2.	Kafka Consumer:
/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --bootstrap-server sandbox.hortonworks.com:6667 --topic iris --from-beginning

3.	Kafka Producer’dan örnek mesajlar yazarak çalışırlığını kontrol etme


Nifi Arayüzünde Veri Akışı Oluşturma

1. Nifi arayüzüne git

2. Yukarı menüden bir adet Process Group oluştur. (Sürükle bırak) İsmine MysqlToKafka verelim.


3. Çift tıklayıp içine girelim.

4. 1. Processor QueryDatabaseTable

Add Processor -> QueryDatabaseTable oluştur

5. Sağ tıkla -> Configure -> Properties sekmesi

	DatabaseConnectionPoolingService için yukarıda oluşturduğumuz connection poolu seçiyoruz.
	Database Type= Generic
	Table Name = iris_with_pk
	Columns to return = Hepsini seçmek istiyorsak boş kalıyor.
	Maximum-value columns = Id (incremental bir number olmalı)
	Max Wait time = 0
	Fetch size = 2
	Max rows per flow file = 2
	Output batch size = 2 
	Geri kalan özellikler varsayılan
	

6. 2. processor ConvertAvroToJson
	QueryDatabaseTable Processoru'nü ConvertAvroToJson'a bağlıyoruz.
	
7. 3. processor PublishKafka_0_10 

ConvertAvroToJson PublishKafka_0_10 bağla ve success kutucuğunu işaretle

	Sağ tıkla configure 
	
	Kafka brokers = sandbox-hdp.hortonworks.com:6667
	topic name = iris
	
	Settings tabından failure ve success kutucuklarını işaretle
	
	
Çalıştırma

iris topiğini dinleyen Kafka-Console Consumer açık iken 


Tek tek processor'leri sağ tıklayıp başlatalım.









