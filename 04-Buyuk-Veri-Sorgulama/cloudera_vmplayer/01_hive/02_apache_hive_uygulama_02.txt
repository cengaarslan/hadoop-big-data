========================================================================
			HIVE SHELL İLE VERİ TABANI TEMEL İŞLEMLER
========================================================================
Hive shel veya Dbeaver üzerinden

1. Veri tabanına ait bilgileri öğrenmek

DESCRIBE DATABASE azhadoop;
	/*
	db_name |comment|location                                                         |owner_name|owner_type|parameters|
	--------|-------|-----------------------------------------------------------------|----------|----------|----------|
	azhadoop|       |hdfs://cloudera.impektra.com:8020/user/hive/warehouse/azhadoop.db|murat     |USER      |          |
	*/

2. Veri tabanını silme
DROP DATABASE azhadoop;

3. veri tabanı oluştururken yorum yazma;

create database if not exists azhadoop comment 'This database is for Hadoop training';





========================================================================
			HIVE SHELL İLE TABLO TEMEL İŞLEMLER
========================================================================

1. Internel ve external table olmak üzere iki farklı tablo vardır. 
		Farkları metedata'larının nasıl tutulduğuna göre değişir. 

		Internal table için hem metadata hem gerçek data hive tarafından yönetilir ve saklanırken external table da sadece metadata 
		hive tarafından tutulur.
		Eğer bir internal table drop edilirse hem metadata hem data ikisi de silinir.
		Şayet external table drop edilirse sadece metadata silinir veri hdfs'de kalır.
		 
		Varsayılan olarak hive internal table yaratır.
 
2. Tablo işlemlerini hangi veritabanında yapacak isek seçmeliyiz.
use azhadoop;
		OK
		Time taken: 0.227 seconds

3. Tablo yaratma
create table if not exists deneme_tablo1 (col1 string, col2 array<string>, col3 string, col4 int)
row format delimited fields terminated by ',' collection items terminated by ':' lines terminated by '\n' stored as textfile;



	3.1. Komut açıklama:
	row format	delimited				: satır formatı; belli karakterlerle sütunlar sınırlandırılmıştır.
	fields terminated by ','			: sütunlar "," ile ayrılmış demektir.
	collection items terminated by ':'	: sütunlardan biri array içeriyor ise array elemenları ':' ile ayrılmıştır.
	lines terminated by '\n'			: her yeni satır başı yeni bir satırdır.

	3.2.
	row format delimited fields terminated by ',' collection items terminated by ':' lines terminated by '\n'  stored as textfile;
	bu komutlar aynı zamanda varsayılan değerler olduğundan atlanabilir.

4. varsayılan değerlerle tablo yaratma

create table if not exists deneme_tablo2 (col1 string, col2 array<string>, col3 string, col4 int);



5. Hive tablolarının HDFS adresleri vardır. Varsayılan internal tablo dizinleri 
	/user/hive/warehouse dizinidir.

	Farklı bir dizin kullanmak için bunu tablo oluştururken belirtiriz.

create table if not exists deneme_tablo3 (col1 string, col2 array<string>, col3 string, col4 int) 
row format delimited fields terminated by ',' collection items terminated by ':' 
lines terminated by '\n'  stored as textfile location '/user/hive/deneme_tablo3';


		Not komut hive shell üzerinden çalıştırılıyorsa hedef dizinde kullanıcı yazma yetkisi olmalıdır.




6. HDFS'deki bir dosyadan hive tablosuna veri yükleme. Aslında burada yapılan iş dosya ile metadata eşleştirmesidir.

	13.1. table1.txt dosyasını Ambari File View ile /user/admin dizinine yükleyiniz.
	
	13.2.  load data inpath '/user/admin/table1.txt' into table table1;
		Loading data to table azhadoop.table1
		Failed with exception org.apache.hadoop.security.AccessControlException: Permission denied. user=maria_dev is not the owner of inode=table1.txt
				at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkOwner(FSPermissionChecker.java:285)
				at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:260)
				at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkDefaultEnforcer(RangerHdfsAuthorizer.java:427)
				at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:364)
				at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
				at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)
				at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1934)
				at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkOwner(FSDirectory.java:1903)
				at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:82)
				at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1877)
				at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:828)
				at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:476)
				at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
				at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
				at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
				at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
				at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
				at java.security.AccessController.doPrivileged(Native Method)
				at javax.security.auth.Subject.doAs(Subject.java:422)
				at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
				at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

		FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask

		Bu hatanın sebebi maria_dev kullanıcısının bu dosyaya sahip olmaması ve hadoop kullanıcı grubunda bulunmaması.
		
		Ancak her ne kadar hata da alsa verinin yüklendiğini görüyoruz.

select * from table1 limit 3;
		OK
		499     ["Poole","GBR"] England 141000
		501     ["Blackburn","GBR"]     England 140000
		500     ["Bolton","GBR"]        England 139020
		Time taken: 0.332 seconds, Fetched: 3 row(s)

	/user/admin diznini kontrol ettiğimizde ise dosyanın yerinde olmadığını göreceğiz. Çünkü hive tablosu yaratıldığı ve tablo internal olduğu için
	veri varsayılan dizin olan /apps/hive/warehouse'a gitti
	
	
	13.3. Aynı dosyayı hdfs /user/maria_dev dizinine de yükleyelim.
	
	13.4. admin kullanıcısı ile upload ettiğimiz için yine sahiplik admin olacaktır. Komut satırından bunu değiştirelim.
	hive shell'den 
quit; 
	ile çıkalım.
sudo -u hdfs hdfs dfs -chown maria_dev:hdfs /user/maria_dev/table1.txt
	şimdi sahiplik değişti.
	
	hive shell'e bağlanalım.
	
hive

	veri tabanı seçelim
use azhadoop;

	13.5. Tekrar yükleyelim
load data inpath '/user/maria_dev/table1.txt' into table table1;

		Loading data to table azhadoop.table1
		Failed with exception org.apache.hadoop.security.AccessControlException: User null does not belong to hadoop
				at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:89)
				at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1877)
				at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:828)
				at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:476)
				at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
				at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
				at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
				at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
				at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
				at java.security.AccessController.doPrivileged(Native Method)
				at javax.security.auth.Subject.doAs(Subject.java:422)
				at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
				at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

		FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask
		
		Hata yine aldı çünkü sahiplik değiştirdik ancak grup hadoop yapmadık. Hata almasına rağmen veri tabloya yüklendi.
		
			
	13.6. Bu hatayı düzeltmek için önce tabloyu truncate edelim.
truncate table table1;
	
		Hive shell çıkış:
quit;
		daha sonra /user/maria_dev dizinine table1.txt dosyasını yükleyelim.
	
		yine admin ownership ile yükleyecektir. Onu maria_dev:hadoop yapalım.

sudo -u hdfs hdfs dfs -chown maria_dev:hadoop /user/maria_dev/table1.txt

		Bu sefer hata almadan başarıtla yükledi.
		hive shell'e tekrar bağlanalım.
hive
use azhadoop;

load data inpath '/user/maria_dev/table1.txt' into table table1;
	Loading data to table azhadoop.table1
	Table azhadoop.table1 stats: [numFiles=1, numRows=0, totalSize=185, rawDataSize=0]
	OK
	Time taken: 1.541 seconds


	13.7. kontrol

select * from table1;
		OK
		499     ["Poole","GBR"] England 141000
		501     ["Blackburn","GBR"]     England 140000
		500     ["Bolton","GBR"]        England 139020
		502     ["Newport","GBR"]       Wales   139000
		503     ["PrestON","GBR"]       England 135000
		504     ["Stockport","GBR"]     England 132813
		Time taken: 0.524 seconds, Fetched: 6 row(s)


quit;













